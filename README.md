# Kleinanzeigen Notebook Scraper

A full-stack application that scrapes notebook listings from Kleinanzeigen, stores normalized data, and exposes it via a REST API and React frontend for search, filters, and analysis.

> âš ï¸ **Disclaimer:** This project is for personal research and educational use only. Always respect Kleinanzeigen's Terms of Service and robots.txt before scraping.

## Features

- ğŸ” **Search & Filter** - Find notebooks by keyword, price range, location, and condition
- ğŸ“Š **Market Statistics** - View average prices, price distribution, and top cities
- ğŸ¤– **Automated Scraping** - Playwright-powered scraper with rate limiting and robots.txt compliance
- ğŸ“± **Responsive UI** - Modern React frontend with TailwindCSS
- ğŸ³ **Docker Ready** - Full-stack deployment with Docker Compose

## Quick Start

### Prerequisites

- Python 3.12+
- Node.js 20+
- PostgreSQL (or use SQLite for development)
- Docker & Docker Compose (optional)

### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Run development server
flask --app app run --debug --port 5000
```

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Run development server
npm run dev
```

### Docker Setup

```bash
# Start full stack
docker compose up --build

# Stop
docker compose down
```

## Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py          # Flask entrypoint and API routes
â”‚   â”œâ”€â”€ config.py       # Environment configuration
â”‚   â”œâ”€â”€ models.py       # SQLAlchemy database models
â”‚   â”œâ”€â”€ scraper.py      # Playwright + BeautifulSoup scraper
â”‚   â”œâ”€â”€ tests/          # Pytest test suite
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/ # React components
â”‚   â”‚   â”œâ”€â”€ pages/      # Page components
â”‚   â”‚   â”œâ”€â”€ hooks/      # React Query hooks
â”‚   â”‚   â””â”€â”€ services/   # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ WIKI.md            # Architecture documentation
â”œâ”€â”€ BUGS.md            # Bug tracker
â””â”€â”€ README.md
```

## API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/health` | Health check |
| GET | `/api/v1/listings` | List listings (paginated, filterable) |
| GET | `/api/v1/listings/{id}` | Get single listing |
| GET | `/api/v1/stats` | Get aggregate statistics |
| GET | `/api/v1/scraper/jobs` | List scraper jobs |
| POST | `/api/v1/scraper/jobs` | Trigger new scraper job |
| GET | `/api/v1/scraper/jobs/{id}` | Get job status |

## Configuration

Copy `.env.example` to `.env` and configure:

```env
# Flask
SECRET_KEY=your-secret-key
DATABASE_URL=postgresql+psycopg2://user:pass@localhost:5432/kleinanzeigen

# Scraper
SCRAPER_PAGE_LIMIT=5
SCRAPER_DELAY_SECONDS=3.0

# CORS
CORS_ORIGINS=http://localhost:5173
```

## Testing

```bash
# Backend tests
cd backend
pytest
pytest --cov=. --cov-report=html

# Frontend
cd frontend
npm run lint
npm run type-check
```

## Documentation

- [WIKI.md](./WIKI.md) - Architecture and API documentation
- [BUGS.md](./BUGS.md) - Bug tracker and fix history

## License

MIT License - See LICENSE file for details.
