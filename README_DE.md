# Kleinanzeigen Notebook Scraper

Eine Full-Stack-Anwendung, die Notebook-Anzeigen von Kleinanzeigen scrapt, normalisierte Daten speichert und diese Ã¼ber eine REST-API und ein React-Frontend fÃ¼r Suche, Filterung und Analyse bereitstellt.

> âš ï¸ **Haftungsausschluss:** Dieses Projekt dient ausschlieÃŸlich persÃ¶nlichen Forschungs- und Bildungszwecken. Beachten Sie vor dem Scraping stets die Nutzungsbedingungen und die robots.txt von Kleinanzeigen.

## Funktionen

- ğŸ” **Suche & Filter** - Finden Sie Notebooks nach Stichwort, Preisspanne, Standort und Zustand
- ğŸ“Š **Marktstatistiken** - Durchschnittspreise, Preisverteilung und Top-StÃ¤dte anzeigen
- ğŸ¤– **Automatisches Scraping** - Playwright-basierter Scraper mit Ratenbegrenzung und robots.txt-KonformitÃ¤t
- ğŸ“± **Responsive UI** - Modernes React-Frontend mit TailwindCSS und Framer Motion (2025 Design)
- ğŸ³ **Docker Ready** - Full-Stack-Deployment mit Docker Compose

## Schnellstart

### Voraussetzungen

- Python 3.12+
- Node.js 20+
- PostgreSQL (oder SQLite fÃ¼r die Entwicklung nutzen)
- Docker & Docker Compose (optional)

### Backend-Einrichtung

```bash
cd backend

# Virtuelle Umgebung erstellen
python -m venv .venv
.\.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt

# Playwright Browser installieren
playwright install chromium

# Entwicklungsserver starten
flask --app app run --debug --port 5000
```

### Frontend-Einrichtung

```bash
cd frontend

# AbhÃ¤ngigkeiten installieren
npm install

# Entwicklungsserver starten
npm run dev
```

### Docker-Einrichtung

```bash
# Full-Stack starten
docker compose up --build

# Stoppen
docker compose down
```

## Projektstruktur

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py          # Flask Einstiegspunkt und API-Routen
â”‚   â”œâ”€â”€ config.py       # Umgebungskonfiguration
â”‚   â”œâ”€â”€ models.py       # SQLAlchemy Datenbankmodelle
â”‚   â”œâ”€â”€ scraper.py      # Playwright + BeautifulSoup Scraper
â”‚   â”œâ”€â”€ tests/          # Pytest Test-Suite
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/ # React Komponenten
â”‚   â”‚   â”œâ”€â”€ pages/      # Seiten-Komponenten
â”‚   â”‚   â”œâ”€â”€ hooks/      # React Query Hooks
â”‚   â”‚   â””â”€â”€ services/   # API Client
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ WIKI.md            # Architekturdokumentation
â”œâ”€â”€ BUGS.md            # Bug-Tracker
â””â”€â”€ README.md
```

## API-Endpunkte

| Methode | Endpunkt | Beschreibung |
|---------|----------|--------------|
| GET | `/api/health` | GesundheitsprÃ¼fung |
| GET | `/api/v1/listings` | Anzeigen auflisten (paginiert, filterbar) |
| GET | `/api/v1/listings/{id}` | Einzelne Anzeige abrufen |
| GET | `/api/v1/stats` | Aggregierte Statistiken abrufen |
| GET | `/api/v1/scraper/jobs` | Scraper-Jobs auflisten |
| POST | `/api/v1/scraper/jobs` | Neuen Scraper-Job starten |
| GET | `/api/v1/scraper/jobs/{id}` | Job-Status abrufen |

## Konfiguration

Kopieren Sie `.env.example` nach `.env` und konfigurieren Sie:

```env
# Flask
SECRET_KEY=ihr-geheimer-schluessel
DATABASE_URL=postgresql+psycopg2://user:pass@localhost:5432/kleinanzeigen

# Scraper
SCRAPER_PAGE_LIMIT=5
SCRAPER_DELAY_SECONDS=3.0

# CORS
CORS_ORIGINS=http://localhost:5173
```

## Testen

```bash
# Backend-Tests
cd backend
pytest
pytest --cov=. --cov-report=html

# Frontend
cd frontend
npm run lint
npm run type-check
```

## Dokumentation

- [WIKI.md](./WIKI.md) - Architektur- und API-Dokumentation
- [BUGS.md](./BUGS.md) - Bug-Tracker und Fix-Historie

## Lizenz

MIT Lizenz - Siehe LICENSE-Datei fÃ¼r Details.
